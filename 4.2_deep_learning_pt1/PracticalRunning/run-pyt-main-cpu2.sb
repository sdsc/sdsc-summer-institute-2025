#!/usr/bin/env bash
#SBATCH --job-name=pyt-cpu
#SBATCH --account=gue998
#SBATCH --partition=compute
#SBATCH --nodes=2  #try 1 or 2  
#SBATCH --ntasks-per-node=8 #8 or 16 or 32 etc..  
#SBATCH --cpus-per-task=1   
#SBATCH --mem=243G   
#SBATCH --time=00:35:00
#SBATCH --output=slurm.cpu1x8.%x.%N.%j.out

module reset
module load slurm  
module load gcc/10.2.0          #compiler, unix   
module load openmpi/4.1.3       #open mpi       
module load singularitypro      #container      
module list

#These are suggested for expanse
export OMPI_MCA_btl='self,vader'
export UCX_TLS='shm,rc,ud,dc'
export UCX_NET_DEVICES='mlx5_0:1'
export UCX_MAX_RNDV_RAILS=1

#set up ip addresses for communication
declare -xr MASTER_ADDR=$(mpirun --allow-run-as-root -n 1 hostname -i 2>/dev/null |tail -n 1);
declare -xr MASTER_PORT=${MASTER_PORT:-15566};

#use -n num-of-nodes * num-per-node
mpirun -n 16 -npernode 8  singularity exec --bind /expanse,/scratch  /cm/shared/apps/containers/singularity/pytorch/pytorch-latest.sif python3 ./mnist_multinode_v6.py > stdout_mnist_multinode_cpu_$SLURM_NTASKS_PER_NODE.txt



